##### EDITABLE CONFIGS #####

## Main Training Params
optimizer: adamw_8bit
learning_rate: 2e-4
gradient_accumulation_steps: 1
micro_batch_size: 64
eval_batch_size: 64
weight_decay: 0.01
gradient_checkpointing: true
packing: true
use_liger_kernel: true

adapter: lora
lora_r: 32
lora_alpha: 32
lora_dropout: 0
lora_target_linear: true
beta: 0.2
use_neftune: false

early_stopping: true
early_stopping_patience: 4
greater_is_better: false
metric_for_best_model: eval_loss
eval_strategy: steps

max_steps: 10000
save_steps: 100
eval_steps: 50
warmup_steps: 100
save_strategy: steps
logging_steps: 20
save_total_limit: 5

cleanlab: true               # turn filtering on/off
cleanlab_keep_frac: 0.90    # keep top-92 % (empirically sweet spot)
embed_model: sentence-transformers/all-MiniLM-L6-v2
embed_batch: 128

dataloader_num_workers: 8
val_set_size: 0.05
sequence_len: 8196

##### AUTOSET PARAMS #####
model_params_count: 0
task_id:
rl:
datasets:
dataset_prepared_path:
output_dir: training_output




