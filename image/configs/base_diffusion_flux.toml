# =========================
# FLUX LoRA — Low-L2 Config
# =========================

# 1) Model & paths
pretrained_model_name_or_path = "/app/flux/unet.safetensors"
ae      = "/app/flux/ae.safetensors"
clip_l  = "/app/flux/clip_l.safetensors"
t5xxl   = "/app/flux/t5xxl_fp16.safetensors"
apply_t5_attn_mask = true

# 2) Objective & determinism
loss_type = "l2"
seed = 1
model_prediction_type = "raw"

# 3) Optimizer & schedule (stable fit > fancy)
optimizer_type = "AdamW8bit"
optimizer_args = ["weight_decay=0.01", "betas=0.9,0.999"]
lr_scheduler = "constant_with_warmup"
lr_warmup_steps = 100

unet_lr = 0.0002
text_encoder_lr = [0, 0]
max_grad_norm = 1.0

# 4) LoRA targets
network_module = "networks.lora_flux"
network_dim = 64
network_alpha = 32
network_args = ["train_double_block_indices=all", "train_single_block_indices=all", "train_t5xxl=False", "train_clip_l=False",]

# 5) Data & bucketing
no_metadata = true
enable_bucket = true
bucket_no_upscale = true
resolution = "1024,1024"
bucket_reso_steps = 64
min_bucket_reso = 256
max_bucket_reso = 2048
caption_extension = ".txt"
train_data_dir = ""

# 6) Precision & memory
mixed_precision = "bf16"
gradient_checkpointing = true
gradient_accumulation_steps = 1
train_batch_size = 1
cache_latents = true
cache_latents_to_disk = true
highvram = true
mem_eff_save = true
xformers = true

# 7) Turn off “look nicer but shift pixels” tricks
noise_offset_type = "None"
multires_noise_iterations = 0
multires_noise_discount = 0.0

# 8) Training length & saving (pick checkpoint by val-L2)
epoch = 0                               # drive by steps
max_train_steps = 6000
save_every_n_steps = 500
save_model_as = "safetensors"
save_precision = "bf16"
prior_loss_weight = 0

# 9) Outputs
output_dir = "/app/outputs"
output_name = "last"
sample_sampler = "euler_a"              # for quick sanity samples; doesn’t affect training
