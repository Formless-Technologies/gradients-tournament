# ===========================
# SDXL LoRA — Low-L2 Config
# ===========================

# 1) Base model
pretrained_model_name_or_path = "stabilityai/stable-diffusion-xl-base-1.0"

# 2) Objective & determinism
loss_type = "l2"
seed = 1
max_grad_norm = 1.0

# 3) Optimizer & schedule (stable fit)
optimizer_type = "AdamW8bit"
optimizer_args = ["weight_decay=0.01", "betas=0.9,0.999"]
lr_scheduler = "constant_with_warmup"
lr_warmup_steps = 100

# Learning rates
learning_rate = 0.0002                 # global default
unet_lr = 0.0002
text_encoder_lr = 0                    # freeze TE for strict pixel fidelity

# 4) LoRA setup (include 3×3 conv)
network_module = "networks.lora"
network_dim = 32
network_alpha = 16
network_args = ["conv_dim=32", "conv_alpha=16"]
network_train_unet_only = true         # <— keep LoRA only on U-Net

# 5) Data & captions
no_metadata = true
caption_extension = ".txt"
shuffle_caption = false
keep_tokens = 0

# 6) Bucketing (match eval shapes; no upscaling)
enable_bucket = true
bucket_no_upscale = true
resolution = "1024,1024"
bucket_reso_steps = 64
min_bucket_reso = 256
max_bucket_reso = 2048

# 7) Precision & memory
mixed_precision = "bf16"
no_half_vae = true
gradient_checkpointing = true
gradient_accumulation_steps = 1
train_batch_size = 1
cache_latents = true
cache_latents_to_disk = true
highvram = true
xformers = true

# 8) Disable tricks that shift tone/contrast (bad for L2)
min_snr_gamma = 0
noise_offset_type = "None"
multires_noise_iterations = 0
multires_noise_discount = 0.0
max_token_length = 125

# 9) Training length & saving (pick best by val-L2)
epoch = 0                              # drive by steps
max_train_steps = 8000
save_every_n_steps = 500
save_model_as = "safetensors"
save_precision = "bf16"
prior_loss_weight = 0
scale_weight_norms = 0

# 10) Output
output_dir = "/app/outputs"
output_name = "last"
train_data_dir = ""
sample_sampler = "euler_a"             # for sanity previews only
